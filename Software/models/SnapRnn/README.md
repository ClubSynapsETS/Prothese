# SnapRnn
------

### Global view
**SnapRnn provides a pipeline for real time finger movement recognition. The pipeline takes emg readings from live sensors as inputs and outputs movement types.**

What is described as a pipeline is a set of processes running simultaneously, launched via  functions from this module. The processes include a data acquisition process, several RNN models running in parallel and a prediction evaluation process. At the end of the pipeline we can collect the classifications for the fed data. 

The module also provides a set of tools to create, tweek, train and evaluate a recursive neural network (RNN). Said Neural network must be fed emg data and output movement classifications.

Note that this module is under development and that the current LIVE system performs poorly.

### More on the RNN and input data: 

The RNN model used in this module is defined in the model.py.

In the examples, when the RNN is to be evaluated or trained on pre-existing data, the data is pulled from .json files. The **ModelBatchGenerator** class provided by this module allows to pull data from several .json files and feed the RNN.

All the training data we currently have was generated by a tool called Pewter (see link below) which allows the capture of emg data from the myo armband (from thalmic labs).

The Pewter tool outputs the captured data in .json files with a particular structure. The **ModelBatchGenerator** class is made (hardcoded) to pull data from .json files with the same particular structure. This will most likely change in the future as we change are acquisition methods.

Pewter project link : https://github.com/Neural-Space/pewter

### Training/Testing data layout
+ Training_data/
	+ movement1_name/
    	+ movement1_name_1_1.json
    	+ movement1_name_1_2.json
    	+ movement1_name_1_3.json
	+ movement2_name/
    	+ movement2_name_1_1.json
    	+ movement2_name_1_2.json
    	+ movement2_name_1_3.json

### Recommended workspace layout
+ Workspace/
	+ Training_data/
		+ movement1_name/
			+ movement1_name_1_1.json
			+ movement1_name_1_2.json
			+ movement1_name_1_3.json
		+ movement2_name/
			+ movement2_name_1_1.json
			+ movement2_name_1_2.json
			+ movement2_name_1_3.json
	+ Testing_data/
		+ movement1_name/
			+ movement1_name_1_1.json
			+ movement1_name_1_2.json
			+ movement1_name_1_3.json
		+ movement2_name/
			+ movement2_name_1_1.json
			+ movement2_name_1_2.json
			+ movement2_name_1_3.json
	+ Model/
		+ model files … 
	+ tf_logs/
		+ tensorflow log files …
	+ model_train_ex.py
	+ param_search_ex.py
	+ confusion_matrix_ex.py
	+ single_model_eval_ex.py
	+ movement_recognition_ex.py
	+ Example_calibration_file.json
	+ Testing_data.json
	+ Training_data.json

### Example files explanation

##### model_train_ex.py 
Generates a blank RNN model based on the provided parameters, then trains it with the data in the specified training folder. During training, the accuracy on the training and testing data is logged in the tf_logs folder. To visualize the training logs, launch TensorBoard with logdir = tf_logs/

##### param_search_ex.py
Generates n model parameter sets where the parameter values are randomly generated but anchored around a provided value. For each parameter set, a RNN is built and trained and its maximum accuracy is logged. The user can then look at the log and see which parameter set yields the best results.

##### confusion_matrix_ex.py
Requires a pre-trained model to reside in the Model/ folder. The example instantiates an RNN model, fetches the testing data set and generates a confusion matrix based on the model’s performance. The confusion matrix is generated as an image and saved in the current working directory.

##### single_model_eval_ex.py
Requires a pre-trained model to reside in the Model/ folder. The example instantiates a model and feeds it data from live emg sensors. The prediction made by the model for every step is printed out to the monitor. This is a good way of checking the performance of an individual RNN.

##### movement_recognition_ex.py
Requires a pre-trained model to reside in the Model/ folder. The example instantiates several RNN instances along with a data acquisition process and a prediction evaluation process. The code aims to perform live movement classification. Currently the example performs poorly mostly due to bad training data quality and a rudimentary prediction evaluation algorithm.

### System requirements
+ linux (ubuntu)
+ Python3.6
+ pip3

### Python requirements

+ Requires the “myo_read_multi” package to be installed.
Link to package : https://github.com/ClubSynapsETS/Prothese/tree/master/Software/communication/Myo_read_multi

+ Requires the ”Calibration_Pipeline” package to be installed.
Link to package : https://github.com/ClubSynapsETS/Prothese/tree/master/Software/data_processing/Calibration_Pipeline

+ $pip3 install -r requirements.txt

### Installing the SnapRnn package
	copy the "snaprnn" folder next to the "exemple.py" file
	paste the folder in the wanted location (ex : downloaded modules folder) 
	from the command line, navigate to the pasted folder
	$ pip3 install --user -e snaprnn